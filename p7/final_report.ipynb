{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Identify Fraud from Enron Email\n",
    "<b>by Daniel J. Lee</b> <br>\n",
    "<b>July 17, 2017</b>\n",
    "\n",
    "### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The purpose of this project is to identify Enron employees who have commited fraud based on past financial and email datasets. Enron Corporation is a American energy company that faced bankruptcy after being charged with manipulating financial data. We define these employees as a <b>person of interest (POI)</b> if the following was:\n",
    "    \n",
    "    * was indicted\n",
    "    * testified in exchange for immunity\n",
    "    * reached a settlement with the government\n",
    "\n",
    "An example of a POI in this case would be the former CEO, [Jeffrey Skillings](https://en.wikipedia.org/wiki/Jeffrey_Skilling), who attempted to meet Wall Street expectations by modifying its balance sheet to indicate \"favorable performance\".\n",
    "\n",
    "In this project, we used various machine learning algorithms and techniques  to determine <b>how well</b> we can identify a POI with provided financial and email data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, an initial examination of the data is performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>email_address</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>201955</td>\n",
       "      <td>2902</td>\n",
       "      <td>2869717</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "      <td>4175000</td>\n",
       "      <td>126027</td>\n",
       "      <td>1407</td>\n",
       "      <td>-126027</td>\n",
       "      <td>1729541</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2195</td>\n",
       "      <td>152</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>304805</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>477</td>\n",
       "      <td>566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916197</td>\n",
       "      <td>4046157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1757552</td>\n",
       "      <td>465</td>\n",
       "      <td>-560222</td>\n",
       "      <td>5243487</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>864523</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    salary to_messages deferral_payments total_payments  \\\n",
       "ALLEN PHILLIP K     201955        2902           2869717        4484442   \n",
       "BADUM JAMES P          NaN         NaN            178980         182466   \n",
       "BANNANTINE JAMES M     477         566               NaN         916197   \n",
       "\n",
       "                   exercised_stock_options    bonus restricted_stock  \\\n",
       "ALLEN PHILLIP K                    1729541  4175000           126027   \n",
       "BADUM JAMES P                       257817      NaN              NaN   \n",
       "BANNANTINE JAMES M                 4046157      NaN          1757552   \n",
       "\n",
       "                   shared_receipt_with_poi restricted_stock_deferred  \\\n",
       "ALLEN PHILLIP K                       1407                   -126027   \n",
       "BADUM JAMES P                          NaN                       NaN   \n",
       "BANNANTINE JAMES M                     465                   -560222   \n",
       "\n",
       "                   total_stock_value           ...           loan_advances  \\\n",
       "ALLEN PHILLIP K              1729541           ...                     NaN   \n",
       "BADUM JAMES P                 257817           ...                     NaN   \n",
       "BANNANTINE JAMES M           5243487           ...                     NaN   \n",
       "\n",
       "                   from_messages   other from_this_person_to_poi    poi  \\\n",
       "ALLEN PHILLIP K             2195     152                      65  False   \n",
       "BADUM JAMES P                NaN     NaN                     NaN  False   \n",
       "BANNANTINE JAMES M            29  864523                       0  False   \n",
       "\n",
       "                   director_fees deferred_income long_term_incentive  \\\n",
       "ALLEN PHILLIP K              NaN        -3081055              304805   \n",
       "BADUM JAMES P                NaN             NaN                 NaN   \n",
       "BANNANTINE JAMES M           NaN           -5104                 NaN   \n",
       "\n",
       "                                 email_address from_poi_to_this_person  \n",
       "ALLEN PHILLIP K        phillip.allen@enron.com                      47  \n",
       "BADUM JAMES P                              NaN                     NaN  \n",
       "BANNANTINE JAMES M  james.bannantine@enron.com                      39  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 observations in the dataset composed of 18 POIs and 128 Non-POIs.\n"
     ]
    }
   ],
   "source": [
    "print \"There are \" + str(len(df)) + \" observations in the dataset composed of \" \\\n",
    "+ str(len(df[df['poi'] == True])) + \" POIs and \" + str(len(df[df['poi'] == False])) + \" Non-POIs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Overall Features: 21\n",
      "Count of Financial Features: 14\n",
      "Count of Email Features: 6\n"
     ]
    }
   ],
   "source": [
    "print \"Count of Overall Features: \" + str(len(features_list))\n",
    "print \"Count of Financial Features: \" + str(len(financial_features))\n",
    "print \"Count of Email Features: \" + str(len(email_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[Financial Features]</b> <br>\n",
    "salary<br>\n",
    "deferral_payments<br>\n",
    "total_payments<br>\n",
    "loan_advances<br>\n",
    "bonus<br>\n",
    "restricted_stock_deferred<br>\n",
    "deferred_income<br>\n",
    "total_stock_value<br>\n",
    "expenses<br>\n",
    "exercised_stock_options<br>\n",
    "other<br>\n",
    "long_term_incentive<br>\n",
    "restricted_stock<br>\n",
    "director_fees<br>\n",
    "\n",
    "<i>** All units are in US dollars</i>\n",
    "\n",
    "<b>[Email Features]</b> <br>\n",
    "to_messages : number of emails received<br>\n",
    "from_messages : number of emails sent<br>\n",
    "from_poi_to_this_person : number of emails received from POI<br>\n",
    "from_this_person_to_poi : numbef of emails sent to POI<br>\n",
    "shared_receipt_with_poi : number of emails received with POI<br>\n",
    "email_address: unique string<br>\n",
    "\n",
    "<i>** All units are integers except email_address</i>\n",
    "\n",
    "<b>[POI Indicator]</b> <br>\n",
    "poi : 1 = <b>POI</b> , 0 = <b>non-POI</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary                        51\n",
       "to_messages                   60\n",
       "deferral_payments            107\n",
       "total_payments                21\n",
       "exercised_stock_options       44\n",
       "bonus                         64\n",
       "restricted_stock              36\n",
       "shared_receipt_with_poi       60\n",
       "restricted_stock_deferred    128\n",
       "total_stock_value             20\n",
       "expenses                      51\n",
       "loan_advances                142\n",
       "from_messages                 60\n",
       "other                         53\n",
       "from_this_person_to_poi       60\n",
       "poi                            0\n",
       "director_fees                129\n",
       "deferred_income               97\n",
       "long_term_incentive           80\n",
       "email_address                 35\n",
       "from_poi_to_this_person       60\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x25AA; All of the features (excluding poi) are missing values. We decided to not adjust any data through methods such as filling with null values with the mean in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Outlier Investigation\n",
    "\n",
    "The main files used to investigate outliers were <b>outlier_classification.py</b> and <b>enron61702insiderpay.pdf</b>. We developed plots in <b>outlier_classification.py</b> to determine various financial features outliers. The following points came about-\n",
    "\n",
    "#### \"TOTAL\" and \"THE TRAVEL AGENCY IN THE PARK\"\n",
    "\n",
    "While checking 'salary', there was a single outlier of <i>TOTAL</i> that exceeded 20 million USD. <i>THE TRAVEL AGENCY IN THE PARK</i> was discovered and determined it was not an employee.\n",
    "\n",
    "#### \"BHATNAGAR SANJAY\" and \"BELFER ROBERT\"\n",
    "\n",
    "While checking <i>restricted_stock_deferred</i>, there were large positive outliers whereas the .pdf file indicated all negative values. After reviewing, it was shown that there was a shift in the financial feature information for two employees: <i>BHATNAGAR SANJAY</i> and <i>BELFER ROBERT</i>. The following below is after reshifting with the provided code located in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary                                   0\n",
       "to_messages                            523\n",
       "deferral_payments                        0\n",
       "total_payments                      137864\n",
       "exercised_stock_options           15456290\n",
       "bonus                                    0\n",
       "restricted_stock                   2604490\n",
       "shared_receipt_with_poi                463\n",
       "restricted_stock_deferred         -2604490\n",
       "total_stock_value                 15456290\n",
       "expenses                            137864\n",
       "loan_advances                            0\n",
       "from_messages                           29\n",
       "other                                    0\n",
       "from_this_person_to_poi                  1\n",
       "poi                                  False\n",
       "director_fees                            0\n",
       "deferred_income                          0\n",
       "long_term_incentive                      0\n",
       "from_poi_to_this_person                  0\n",
       "received_from_poi_ratio                  0\n",
       "sent_to_poi_ratio                0.0344828\n",
       "shared_receipt_with_poi_ratio     0.885277\n",
       "Name: BHATNAGAR SANJAY, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['BHATNAGAR SANJAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values. [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "\n",
    "## 4. Engineered / Scaled Features\n",
    "\n",
    "The following features were created to assist in determining the POIs:\n",
    "<br>\n",
    "&#x25AA; from_poi_to_this_person / from_messages<br>\n",
    "&#x25AA; from_this_person_to_poi / to_messages <br>\n",
    "&#x25AA; shared_receipt_with_poi / from_messages <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                              1.000000\n",
       "to_messages                      0.108730\n",
       "from_messages                   -0.033982\n",
       "from_poi_to_this_person          0.190460\n",
       "from_this_person_to_poi          0.129619\n",
       "shared_receipt_with_poi          0.240876\n",
       "shared_receipt_with_poi_ratio    0.247879\n",
       "sent_to_poi_ratio                0.323885\n",
       "received_from_poi_ratio          0.148698\n",
       "Name: poi, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ee].corr()['poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the new features, we took the ratio of the employees' <b>messages involving with poi</b> and the <b>total messages either sent/received</b>. The features were created for employees who did <b>not</b> have missing value in the email features for the particular ratio. \n",
    "\n",
    "The given email features were created to allowed us to examine to what extent an employee is actually involved as a POI. It seems reasonable that it gave a better picture of a POI's behavior rather than their individual counts alone.\n",
    "\n",
    "Based on the correlation test above, I decided to not include the <b>from_messages</b> feature since the new features, <b>sent_to_poi_ratio</b> and <b>received_from_poi_ratio</b>, displayed a better correlation with poi than the initial features itself. In addition, there seemed to be no correlation between from_messages and poi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition: Log Scaling\n",
    "\n",
    "Since the financial data tend to show [skewed behavior](https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/), I was interested in seeing whether a log transformation was going to affect the correlation between the financial features and the POI label. The scaling won't be used for the algorithms but I thought it would be interesting to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                          1.000000\n",
       "salary                       0.340120\n",
       "bonus                        0.359381\n",
       "long_term_incentive          0.257361\n",
       "deferred_income             -0.274393\n",
       "deferral_payments           -0.039439\n",
       "loan_advances                0.220295\n",
       "other                        0.170687\n",
       "expenses                     0.193956\n",
       "director_fees               -0.121080\n",
       "total_payments               0.249020\n",
       "exercised_stock_options      0.370618\n",
       "restricted_stock             0.243607\n",
       "restricted_stock_deferred    0.073052\n",
       "total_stock_value            0.371828\n",
       "Name: poi, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ff].corr()['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poi                          1.000000\n",
       "salary                       0.241644\n",
       "bonus                        0.282985\n",
       "long_term_incentive          0.185817\n",
       "deferred_income              0.225283\n",
       "deferral_payments            0.003921\n",
       "loan_advances                0.123489\n",
       "other                        0.337724\n",
       "expenses                     0.294070\n",
       "director_fees               -0.128680\n",
       "total_payments               0.218541\n",
       "exercised_stock_options      0.026145\n",
       "restricted_stock             0.199639\n",
       "restricted_stock_deferred   -0.137413\n",
       "total_stock_value            0.209197\n",
       "Name: poi, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df[ff].corr()['poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations:</b>\n",
    "\n",
    "&#x25AA; <i>salary</i>, <i>bonus</i>, <i>loan_advances</i>, and <i>exercised_stock_options</i> became less relevant <br>\n",
    "&#x25AA; <i>other</i> and <i>expenses</i> became more relevant.. <br>\n",
    "&#x25AA; <i>deferred_income</i> and <i>restricted_stock_deferred</i> changed signs ( - to + )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Selecting Features\n",
    "\n",
    "With the given features, we want to determine a feature's importance to whether an employee is a POI. The univariate feature selection process, [SelectKBest( )](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), was used with GridSearchCV to select the 'k' amount of features that would maximize the precision and recall values.\n",
    "\n",
    "I performed an initial run on the <b>DecisionTreeClassifier()</b> with parameters shown below to determine the optimal amount of features. I started with the default value of 10 features and continually increased to examine the results using file <i>tester.py</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters used to determine the 'k' features\n",
    "kbest__k=[8,9,10,11,12,13,14,15,'all']\n",
    "\n",
    "dtc__criterion=['gini','entropy'],\n",
    "                dtc__min_samples_leaf=[1,2,3],\n",
    "                dtc__max_depth=[2,3,4],\n",
    "                dtc__class_weight=['balanced']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Test 1:  Determining 'k' Features Incl. Engineered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    [Trials incl. engineered features]\n",
    "    \n",
    "    *k=8\n",
    "    Accuracy: 0.81907\tPrecision: 0.38187\tRecall: 0.57700\tF1: 0.45958\tF2: 0.52350\n",
    "    \n",
    "    *k=10\n",
    "    Accuracy: 0.82420\tPrecision: 0.39111\tRecall: 0.57200\tF1: 0.46457\tF2: 0.52357\n",
    "    \n",
    "    *k=12\n",
    "    Accuracy: 0.82920\tPrecision: 0.40403\tRecall: 0.59150\tF1: 0.48011\tF2: 0.54127\n",
    "    \n",
    "    *k=14\n",
    "    Accuracy: 0.82027\tPrecision: 0.39259\tRecall: 0.63600\tF1: 0.48550\tF2: 0.56584\n",
    "    \n",
    "    *k='all'\n",
    "    Accuracy: 0.74487\tPrecision: 0.29874\tRecall: 0.67800\tF1: 0.41474\tF2: 0.54071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x25AA; As we increase 'k' amount of features, we can also see an <b>increase in our recall score</b> but fluctuations in the precision. Note that results were <i>including the new features</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Test 2:  Determining 'k' Features Excl. Engineered Features\n",
    "\n",
    "Before proceeding, we want to determine whether the engineered features had an effect on the precision and recall scores. The same parameters for DecisionTreeClassifier() were used again <i>excluding the engineered features</i>:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [Trials excl. engineered features]\n",
    "    \n",
    "    *k=8\n",
    "    Accuracy: 0.66600\tPrecision: 0.21322\tRecall: 0.55950\tF1: 0.30877\tF2: 0.42233\n",
    "    \n",
    "    *k=10\n",
    "    Accuracy: 0.66273\tPrecision: 0.20376\tRecall: 0.52600\tF1: 0.29373\tF2: 0.39960\n",
    "    \n",
    "    *k=12\n",
    "    Accuracy: 0.70720\tPrecision: 0.25350\tRecall: 0.61500\tF1: 0.35902\tF2: 0.47852\n",
    "    \n",
    "    *k=14\n",
    "    Accuracy: 0.73793\tPrecision: 0.29721\tRecall: 0.70750\tF1: 0.41858\tF2: 0.55442\n",
    "    \n",
    "    *k='all'\n",
    "    Accuracy: 0.73613\tPrecision: 0.31238\tRecall: 0.81500\tF1: 0.45165\tF2: 0.61658"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x25AA; It seems that including the engineered features <i>negatively</i> affects the recall scores. Thus the engineered features were removed from the features list. From the results above, we decided to continue with using <b>k = 'all'</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Selected Features for k='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using k = 'all' and excluding the engineered features..\n",
    "\n",
    "#SelectKBest() Scores: \n",
    "[('bonus', 30.728774633399713),\n",
    " ('salary', 15.858730905995131),\n",
    " ('shared_receipt_with_poi', 10.722570813682712),\n",
    " ('total_stock_value', 10.632581622204556),\n",
    " ('exercised_stock_options', 9.6801736665963212),\n",
    " ('total_payments', 8.9611911472233619),\n",
    " ('deferred_income', 8.7530370568859475),\n",
    " ('restricted_stock', 8.0549780560999462),\n",
    " ('long_term_incentive', 7.5551197773202938),\n",
    " ('loan_advances', 7.0379327981934612),\n",
    " ('from_poi_to_this_person', 4.9586666839661424),\n",
    " ('expenses', 4.1738959725629785),\n",
    " ('other', 3.2044591402721507),\n",
    " ('to_messages', 2.6161830046793662),\n",
    " ('director_fees', 1.8180800771349392),\n",
    " ('restricted_stock_deferred', 0.75787191672004672),\n",
    " ('from_this_person_to_poi', 0.11120823866694469),\n",
    " ('deferral_payments', 0.011021863646593114)]\n",
    "\n",
    "\n",
    "#DecisionTreeClassifier() Feature Importances: \n",
    "[('expenses', 0.73121090112497555),\n",
    " ('shared_receipt_with_poi', 0.26878909887502445),\n",
    " ('salary', 0.0),\n",
    " ('to_messages', 0.0),\n",
    " ('deferral_payments', 0.0),\n",
    " ('total_payments', 0.0),\n",
    " ('exercised_stock_options', 0.0),\n",
    " ('bonus', 0.0),\n",
    " ('director_fees', 0.0),\n",
    " ('restricted_stock_deferred', 0.0),\n",
    " ('total_stock_value', 0.0),\n",
    " ('from_poi_to_this_person', 0.0),\n",
    " ('loan_advances', 0.0),\n",
    " ('other', 0.0),\n",
    " ('from_this_person_to_poi', 0.0),\n",
    " ('deferred_income', 0.0),\n",
    " ('restricted_stock', 0.0),\n",
    " ('long_term_incentive', 0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations</b>:\n",
    "<br>\n",
    "&#x25AA; It seems that our DecisionTreeClassifier() was able to only consider <i>expense</i> and  <i>shared_receipt_with_poi</i> as the only important features.\n",
    "<br>\n",
    "&#x25AA; <i>bonus</i> was the most important feature according to SelectKBest()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Conclusion\n",
    "\n",
    "In this section, we wanted to figure out the optimal amount of features for our initial algorithm. As we performed different values for 'k' amount of features, two separate tests were performed: including and excluding the engineered features. Surprisingly, the engineered features negatively affected the recall score thus removed from the features list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "\n",
    "## 6. Algorithms\n",
    "\n",
    "The following algorithms were performed to obtain the goal of a score <b>higher than 0.3</b> for both precision and recall.\n",
    "    \n",
    "    * SVM / MinMaxScaler()\n",
    "    * Decision Tree Classifier / KBest()\n",
    "    * Random Forest Classifier / KBest()\n",
    "\n",
    "An initial run was conducted to see how each algorithm performs. The file <b>tester.py</b> was used to obtain the precision and recall values for this project:\n",
    "    \n",
    "#### *[SVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "    \n",
    "    *svc__C=[1, 1.5, 1.75, 2, 2.25, 2.5],\n",
    "\t*svc__kernel=['linear', 'sigmoid', 'poly','rbf'],\n",
    "\t*svc__gamma=['auto'],\n",
    "\t*svc__class_weight=[None, 'balanced']\n",
    "    \n",
    "    Accuracy: 0.67140\tPrecision: 0.24685\tRecall: 0.71400\tF1: 0.36686\tF2: 0.51795\n",
    "    \n",
    "    Best parameters:  {'svc__gamma': 'auto',\n",
    "    'svc__class_weight': 'balanced', \n",
    "    'svc__kernel': 'sigmoid',\n",
    "    'svc__C': 2.25}\n",
    "\n",
    "#### *[Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "    \n",
    "    *kbest__k=[4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    *dtc__criterion=['gini','entropy'],\n",
    "    *dtc__min_samples_leaf=[1,2,3],\n",
    "    *dtc__max_depth=[2,3,4],\n",
    "    *dtc__class_weight=['balanced']\n",
    "    \n",
    "    Accuracy: 0.82753\tPrecision: 0.39897\tRecall: 0.57950\tF1: 0.47258\tF2: 0.53141\n",
    "    \n",
    "    Best parameters:  {'dtc__criterion': 'entropy',\n",
    "    'dtc__max_depth': 2,\n",
    "    'kbest__k': 11,\n",
    "    'dtc__min_samples_leaf': 3,\n",
    "    'dtc__class_weight': 'balanced'} \n",
    "\n",
    "#### *[Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "    \n",
    "    *kbest__k=[4,5,6]\n",
    "    *rfc__n_estimators=[9, 11, 12],\n",
    "    *rfc__criterion=['gini', 'entropy'],\n",
    "    *rfc__max_features=['auto', 3, 4],\n",
    "    *rfc__min_samples_split=[2, 3, 4, 5],\n",
    "    \n",
    "    Accuracy: 0.84113\tPrecision: 0.34265\tRecall: 0.20850\tF1: 0.25925\tF2: 0.22621\n",
    "    \n",
    "    Best parameters:  {'rfc__criterion': 'gini', \n",
    "    'rfc__min_samples_split': 2, \n",
    "    'kbest__k': 4,\n",
    "    'rfc__max_features': 'auto',\n",
    "    'rfc__n_estimators': 9} \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). [relevant rubric item: “tune the algorithm”]\n",
    "\n",
    "For each algorithm, parameters are included as shown in the sections above. Tuning an algorithm is optimizing its parameters to favorably affect our model's performance on not just data we already seen. The goal is to have our model be able to make accurate predictions from those parameters. If the model was not appropriately tuned, we would expect weak results with the belief that it is optimal.\n",
    "\n",
    "Although the parameters depend on factors according to the appropriate algorithm, we can still tune them through various approaches. In this project, <b>GridSearchCV</b> was used to tune the algorithms for this project. It allows us to exhaust through sequences of the algorithm's parameters and returns the best choice of fit.\n",
    "\n",
    "In our selected algorithms, we selected few parameters and started with a normal range around the default values. After understanding which direction the model preferred for the parameters, we steadily went in that direction and stopped until the model found an optimal value. Afterwards, we continually added more parameters with ranges and monitored how our results were affected.\n",
    "\n",
    "--\n",
    "\n",
    "Based on the initial run for the algorithm, it is shown the <b>DecisionTreeClassifier()</b> displayed the highest precision and recall values. I decided to further tune the parameters while fixing the best parameters obtained from the initial run:\n",
    "\n",
    "    [Trial 1]\n",
    "    \n",
    "    *dtc__criterion=['gini'],\n",
    "    *dtc__min_samples_leaf=[3],\n",
    "    *dtc__max_depth=[2],\n",
    "    *dtc__class_weight=['balanced'],\n",
    "    *dtc__min_samples_split=[2]\n",
    "\n",
    "    \n",
    "    Accuracy: 0.73773\tPrecision: 0.31425\tRecall: 0.81800\tF1: 0.45407\tF2: 0.61942\n",
    "    \n",
    "    [Trial 2]\n",
    "    \n",
    "    *dtc__criterion=['gini'],\n",
    "    *dtc__min_samples_leaf=[3],\n",
    "    *dtc__max_depth=[2],\n",
    "    *dtc__class_weight=['balanced'],\n",
    "    *dtc__min_samples_split=[2],\n",
    "    *dtc__max_leaf_nodes=[2,3,4,5]\n",
    "    \n",
    "    Accuracy: 0.63087\tPrecision: 0.24890\tRecall: 0.87650\tF1: 0.38770\tF2: 0.58266\n",
    "    \n",
    "    [Trial 3]\n",
    "    \n",
    "    *dtc__criterion=['gini'],\n",
    "    *dtc__min_samples_leaf=[3],\n",
    "    *dtc__max_depth=[2],\n",
    "    *dtc__class_weight=['balanced'],\n",
    "    *dtc__min_samples_split=[4],\n",
    "    *dtc__presort=[False],\n",
    "    *dtc__splitter=['best']\n",
    "    \n",
    "    Accuracy: 0.73793\tPrecision: 0.31458\tRecall: 0.81900\tF1: 0.45456\tF2: 0.62013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis? [relevant rubric item: “validation strategy”]\n",
    "\n",
    "Validating is a method of appropriately measuring the algorithm's performance. A classic error is measuring the algorithm's performance by testing on the <b>same data</b> you trained; we don't want the model to be overfitting to the same training data. By splitting the data into a <i>training and testing set</i>, we are able to verify that our algorithm is able to accurately predict with new data. For this project, we used <b>30%</b> of the data for testing whereas <b>70%</b> of it was used for training.\n",
    "\n",
    "A validation strategy that was used for this project is the [Stratified Shuffle Split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) to create random folds and maintain the distributions of POIs and non-POIs among the training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "## Overall Results:\n",
    "\n",
    "    *dtc__criterion=['gini'],\n",
    "    *dtc__min_samples_leaf=[3],\n",
    "    *dtc__max_depth=[2],\n",
    "    *dtc__class_weight=['balanced'],\n",
    "    *dtc__min_samples_split=[4],\n",
    "    *dtc__presort=[False],\n",
    "    *dtc__splitter=['best']\n",
    "    \n",
    "    Accuracy: 0.73793\tPrecision: 0.31458\tRecall: 0.81900\tF1: 0.45456\tF2: 0.62013\n",
    "    \n",
    "    Total predictions: 15000\n",
    "    True positives: 1445\n",
    "    False positives: 3095\n",
    "    False negatives:  555\n",
    "    True negatives: 9905\n",
    "\n",
    "The goal of this project was to obtain 30% or more in both precision and recall values. We define these terms as follows:\n",
    "\n",
    "&#x25AA; <b> True positives (TP)</b>: the POI employees correctly identified by our model.\n",
    "<br>\n",
    "&#x25AA; <b>True negatives (TN)</b>:  the non-POI employee correctly identified by our model.\n",
    "<br>\n",
    "&#x25AA; <b>False positives (FP)</b>:  the non-POI employee discovered as POI by our model\n",
    "<br>\n",
    "&#x25AA; <b>False negatives (FN)</b>: the POI employee that our model failed to identify as POI.\n",
    "\n",
    "\n",
    "\n",
    "&#x25AA; <b>Precision</b> is the percentage of true POIs found from the total number of POIs.\n",
    "<br>\n",
    "&#x25AA; <b>Recall</b> is the percentage of true POIs from the results identified by our model.\n",
    "\n",
    "\n",
    "In this situation, we ideally want to <b>minimize false negatives</b> because we want to emphasize more about our model not missing POIs than wrongly identifying an employee as POI. Thus in this case, we desire our recall score to be higher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * URL Reference\n",
    "https://en.wikipedia.org/wiki/Enron_scandal<br>\n",
    "https://www.reddit.com/r/explainlikeimfive/comments/1lbvs7/eli5_enron_and_their_scandal/<br>\n",
    "https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/<br>\n",
    "https://stats.stackexchange.com/questions/72231/decision-trees-variable-feature-scaling-and-variable-feature-normalization<br>\n",
    "http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/ <br>\n",
    "http://www.simafore.com/blog/bid/62333/4-key-advantages-of-using-decision-trees-for-predictive-analytics <br>\n",
    "https://en.wikipedia.org/wiki/Overfitting<br>\n",
    "https://www.youtube.com/watch?v=o9A4e7zopu8<br>\n",
    "https://stackoverflow.com/questions/29530232/python-pandas-check-if-any-value-is-nan-in-dataframe<br>\n",
    "https://stackoverflow.com/questions/22903267/what-is-tuning-in-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Python Reference\n",
    "\n",
    "The purpose of the following code was to insert data for the report using pandas for a cleaner format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning Financial and Email Data..\n",
    "\n",
    "features_list = ['poi',\n",
    "'salary',\n",
    "'deferral_payments',\n",
    "'total_payments',\n",
    "'loan_advances',\n",
    "'bonus',\n",
    "'restricted_stock_deferred',\n",
    "'deferred_income',\n",
    "'total_stock_value',\n",
    "'expenses',\n",
    "'exercised_stock_options',\n",
    "'other',\n",
    "'long_term_incentive',\n",
    "'restricted_stock',\n",
    "'director_fees',\n",
    "'to_messages',\n",
    "'email_address',\n",
    "'from_poi_to_this_person',\n",
    "'from_messages',\n",
    "'from_this_person_to_poi',\n",
    "'shared_receipt_with_poi',\n",
    "#'shared_receipt_with_poi_ratio',\n",
    "#'sent_to_poi_ratio',\n",
    "#'received_from_poi_ratio'\n",
    "]\n",
    "\n",
    "financial_features = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', \n",
    "'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments', 'exercised_stock_options', \n",
    "'restricted_stock', 'restricted_stock_deferred', 'total_stock_value']\n",
    "email_features = ['email_address','to_messages', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi',\n",
    "'shared_receipt_with_poi']\n",
    "\n",
    "df = df.replace('NaN', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes the email_address feature since unique\n",
    "df.drop('email_address', axis=1, inplace=True)\n",
    "email_features.remove('email_address')\n",
    "\n",
    "# Removes outliers\n",
    "df.drop('TOTAL', axis=0, inplace=True)\n",
    "df.drop('THE TRAVEL AGENCY IN THE PARK', axis=0, inplace=True)\n",
    "\n",
    "# Replace NaN with 0 to avoid issues\n",
    "df[financial_features] = df[financial_features].replace(np.nan, 0)\n",
    "df[email_features] = df[email_features].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Editing 'BHATNAGAR SANJAY' and 'BELFER ROBERT'\n",
    "df.set_value('BHATNAGAR SANJAY', 'total_payments', 137864);\n",
    "df.set_value('BHATNAGAR SANJAY', 'expenses', 137864);\n",
    "df.set_value('BHATNAGAR SANJAY', 'other', 0);\n",
    "df.set_value('BHATNAGAR SANJAY', 'director_fees', 0);\n",
    "df.set_value('BHATNAGAR SANJAY', 'exercised_stock_options', 15456290);\n",
    "df.set_value('BHATNAGAR SANJAY', 'restricted_stock', 2604490);\n",
    "df.set_value('BHATNAGAR SANJAY', 'restricted_stock_deferred', -2604490);\n",
    "df.set_value('BHATNAGAR SANJAY', 'total_stock_value', 15456290);\n",
    "\n",
    "df.set_value('BELFER ROBERT', 'total_payments', 3285);\n",
    "df.set_value('BELFER ROBERT', 'expenses', 3285);\n",
    "df.set_value('BELFER ROBERT', 'deferred_income', -102500);\n",
    "df.set_value('BELFER ROBERT', 'deferral_payments', 0);\n",
    "df.set_value('BELFER ROBERT', 'director_fees', 102500);\n",
    "df.set_value('BELFER ROBERT', 'exercised_stock_options', 0);\n",
    "df.set_value('BELFER ROBERT', 'restricted_stock', 44093);\n",
    "df.set_value('BELFER ROBERT', 'restricted_stock_deferred', -44093);\n",
    "df.set_value('BELFER ROBERT', 'total_stock_value', 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding new email features...\n",
    "for i in df.index:\n",
    "    if df.loc[i]['from_poi_to_this_person'] == 0 or df.loc[i]['to_messages'] == 0:\n",
    "        df.set_value(i, 'received_from_poi_ratio', 0)\n",
    "    else:\n",
    "        df.set_value(i, 'received_from_poi_ratio',\n",
    "                     float(df.loc[i]['from_poi_to_this_person']) / float(df.loc[i]['to_messages']))\n",
    "\n",
    "for i in df.index:\n",
    "    if df.loc[i]['from_this_person_to_poi'] == 0 or df.loc[i]['from_messages'] == 0:\n",
    "        df.set_value(i, 'sent_to_poi_ratio', 0)\n",
    "    else:\n",
    "        df.set_value(i, 'sent_to_poi_ratio',\n",
    "                     float(df.loc[i]['from_this_person_to_poi']) / float(df.loc[i]['from_messages']))\n",
    "\n",
    "for i in df.index:\n",
    "    if df.loc[i]['shared_receipt_with_poi'] == 0 or df.loc[i]['to_messages'] == 0:\n",
    "        df.set_value(i, 'shared_receipt_with_poi_ratio', 0)\n",
    "    else:\n",
    "        df.set_value(i, 'shared_receipt_with_poi_ratio',\n",
    "                     float(df.loc[i]['shared_receipt_with_poi']) / float(df.loc[i]['to_messages']))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ee = ['poi','to_messages', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi',\n",
    "'shared_receipt_with_poi', 'shared_receipt_with_poi_ratio', 'sent_to_poi_ratio','received_from_poi_ratio']\n",
    "\n",
    "ff = ['poi', 'salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', \n",
    "'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments', 'exercised_stock_options', \n",
    "'restricted_stock', 'restricted_stock_deferred', 'total_stock_value']\n",
    "\n",
    "# What we want to transform from 'ff'\n",
    "filtered_ff = ['salary', 'bonus', 'long_term_incentive', 'deferred_income', 'deferral_payments', \n",
    "'loan_advances', 'other', 'expenses', 'director_fees', 'total_payments', 'exercised_stock_options', \n",
    "'restricted_stock', 'restricted_stock_deferred', 'total_stock_value']\n",
    "\n",
    "log_df = df[ff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in log_df.index:\n",
    "    for f in filtered_ff:\n",
    "        if log_df.loc[i][f] != 0:\n",
    "            log_df.set_value(i,f, np.log(abs(log_df.loc[i][f])))\n",
    "        else:\n",
    "            log_df.set_value(i,f,0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
